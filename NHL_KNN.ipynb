{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahendel1/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nhlRaw = pd.read_csv('NHL Stat DB_2008_2017.csv', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in data from file\n",
    "nhlRaw.columns = [c.replace(' ', '_') for c in nhlRaw.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter to teams that made playoffs\n",
    "# TODO: remove tampa bay\n",
    "nhlRaw = nhlRaw[nhlRaw.CY_Season_Rank < 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the labels from the stat data\n",
    "labels = nhlRaw.iloc[:,0:7]\n",
    "nhlData = nhlRaw.iloc[:,np.r_[0,7:len(nhlRaw.columns)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert values to scaled, group by year\n",
    "nhlScale = nhlData.groupby('Year')\n",
    "# nhlScaled = nhlScale.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "# not sure if z-score is working properly yet\n",
    "nhlScaled = nhlScale.apply(lambda x: (x - np.mean(x)) / np.std(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge the datasets back together if skipping PCA\n",
    "# labels.reset_index(drop=True, inplace=True)\n",
    "# nhlScaled.reset_index(drop=True, inplace=True)\n",
    "# nhlScaled = pd.concat([labels, nhlScaled], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nhlScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now drop year from the dataframe\n",
    "nhlScaled = nhlScaled.drop('Year', 1)\n",
    "nhlScaled = nhlScaled.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nhlCluster = nhlScaled.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cluster stat attriburtes\n",
    "kmeans = KMeans(n_clusters=6, random_state=0).fit(nhlCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# associate the clusters with each stat\n",
    "pcaGroups = pd.DataFrame({'Group' : kmeans.labels_,\n",
    "                          'Stat' : nhlScaled.columns})\n",
    "# assign group names to the dataframe instead of stat names\n",
    "nhlScaled.columns = pcaGroups['Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill NaN with zeros\n",
    "# TODO: find a better imputation method rather than subbing \n",
    "#       a zero.\n",
    "nhlScaled = nhlScaled.fillna(0)\n",
    "# current scaling method is causing some +/- inf values, sub a zero in there too\n",
    "nhlScaled = nhlScaled.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pcaDF = pd.DataFrame()\n",
    "\n",
    "for x in np.unique(pcaGroups['Group']):\n",
    "    # extract a group\n",
    "    pcN = nhlScaled[x]\n",
    "    # PCA on that group\n",
    "    pcN = pca.fit_transform(pcN)\n",
    "    # convert to data frame\n",
    "    pcN = pd.DataFrame(pcN)\n",
    "    # merge that group into dataframe out\n",
    "    pcaDF = pd.concat([pcaDF, pcN], axis = 1)\n",
    "pcaDF.columns = np.unique(pcaGroups['Group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge the datasets back together\n",
    "labels.reset_index(drop=True, inplace=True)\n",
    "pcaDF.reset_index(drop=True, inplace=True)\n",
    "nhlScaled = pd.concat([labels, pcaDF], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill NaN with zeros\n",
    "# TODO: find a better imputation method rather than subbing \n",
    "#       a zero.\n",
    "nhlScaled = nhlScaled.fillna(0)\n",
    "# current scaling method is causing some +/- inf values, sub a zero in there too\n",
    "nhlScaled = nhlScaled.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split to training (history) and test (current year)\n",
    "# traditional methods would split the data 70/30 rather\n",
    "# than prior year and current year\n",
    "\n",
    "# 'Test' set\n",
    "# select current year\n",
    "X_test = nhlScaled[nhlScaled.Year == 2017]\n",
    "# extract the ranks (targets)\n",
    "y_test = X_test.CY_Playoff_Rank\n",
    "# drop ranks from X_\n",
    "X_test = X_test.drop('CY_Playoff_Rank', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save team names, drop the rest of the data\n",
    "X_test_names = X_test.Team\n",
    "X_test = X_test.drop(X_test.columns[[list(range(0,6,1))]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select all prior years for training and validation\n",
    "nhl_train = nhlScaled[nhlScaled.Year < 2017]\n",
    "nhl_train_y = nhl_train.CY_Playoff_Rank\n",
    "\n",
    "nhl_train = nhl_train.drop(nhl_train.columns[[list(range(0,7,1))]], axis = 1)\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(nhl_train,\n",
    "                                                            nhl_train_y,\n",
    "                                                            test_size=0.33, \n",
    "                                                            random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize knn obj.\n",
    "# TODO: optimize num neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors = 5, algorithm = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "# syntax knn.fit(training data, target data)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call the model w/ 'validation' data\n",
    "predicts = knn.predict(X_validate)\n",
    "# convert to dataframe\n",
    "predicts = pd.DataFrame(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge the actual and prediction data frames\n",
    "# reset row indexing to align predictions with actuals\n",
    "y_validate.reset_index(drop=True, inplace=True)\n",
    "predicts.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge the data for presentation\n",
    "results = pd.concat([y_validate, predicts], axis=1)\n",
    "# give name to columns\n",
    "results.columns = ['Actual', 'Predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# view results\n",
    "# knn not good in its current form :(\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_err = np.mean(np.abs(results.Actual - results.Predicted))\n",
    "sd_err = np.std(np.abs(results.Actual - results.Predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean5.549019607843137\n",
      " SD3.8872023585203435\n"
     ]
    }
   ],
   "source": [
    "print('Mean' + str(mean_err) + '\\n SD' + str(sd_err))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
